import os
import sys
from gtts import gTTS
import argparse
import subprocess
import pdb
import gradio as gr
from os.path import join as pjoin
from fun import *
import random
import shutil
from functools import partial

WEBSITE = """
<div class="embed_hidden">
<h1 style='text-align: center'> Text to Video </h1>
<h2 style='text-align: center'>
<nobr>è¾“å…¥æ–‡æœ¬ç”Ÿæˆè§†é¢‘</nobr>
</h2>
<h3> Description </h3>
<p>
ğŸ”¥ğŸ”¥ğŸ”¥ è¯¥é¡µé¢æ˜¯æ•°å­—äººç”Ÿæˆæ¨¡å‹çš„äº¤äº’å¼å±•ç¤º, æ˜¯ä¸€ç§ä»æ–‡æœ¬åˆ°è¯´è¯äººè§†é¢‘çš„æ–¹æ³•!!! å®ƒä¼šæ ¹æ®æ‚¨è¾“å…¥çš„æ–‡æœ¬ç”Ÿæˆç‰¹å®šçš„è¯´è¯äººè§†é¢‘ã€‚ ğŸ”¥ğŸ”¥ğŸ”¥
</p>
<p>
ğŸš€ğŸš€ğŸš€ æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé“¾æ¥å¯ä»¥ä¸‹è½½ç”Ÿæˆçš„è¯´è¯äººè§†é¢‘ï¼Œæ ¼å¼ä¸º <b>MP4</b> , å¯ä»¥æ”¯æŒå¤§å¤šæ•°æ’­æ”¾å™¨ã€‚!!! ğŸš€ğŸš€ğŸš€
</p>
<p>

</p>
</div>
"""
WEBSITE_bottom = """
<div class="embed_hidden">
<p>
We thanks <a href="https://huggingface.co/spaces/Mathux/TMR" target="_blank">TMR</a> for this cool space template.
</p>
</div>
"""

# <p>
# If you have any issues on this space or feature requests, we warmly welcome you to contact us through our <a href="https://github.com/EricGuo5513/momask-codes/issues" target="_blank">repository</a> or <a href="mailto:ymu3@ualberta.ca?subject =[MoMask]Feedback&body = Message">email</a>.
# </p>

# EXAMPLES = [
#    #ä¾‹å­ï¼Œæ–‡æœ¬è¾“å…¥
#    "A person is running on a treadmill.", "The person takes 4 steps backwards.", 
#    "A person jumps up and then lands.", "The person was pushed but did not fall.", 
#    "The person does a salsa dance.", "A figure streches it hands and arms above its head.",
#    "This person kicks with his right leg then jabs several times.",
#    "A person stands for few seconds and picks up his arms and shakes them.",
#    "A person walks in a clockwise circle and stops where he began.",
#    "A man bends down and picks something up with his right hand.",
#    "A person walks with a limp, their left leg gets injured.",
#    "A person repeatedly blocks their face with their right arm.",
#    "The person holds his left foot with his left hand, puts his right foot up and left hand up too.",
#    "A person stands, crosses left leg in front of the right, lowering themselves until they are sitting, both hands on the floor before standing and uncrossing legs.",
#    "The man walked forward, spun right on one foot and walked back to his original position.",
#    "A man is walking forward then steps over an object then continues walking forward.",
# ]

EXAMPLES = [
   #ä¾‹å­ï¼Œæ–‡æœ¬è¾“å…¥
   "ç»™æˆ‘æ’­æ”¾ä¸€ä¸ªåŠ¨åŠ›ç«è½¦å”±çš„æ­Œ.", "æˆ‘è¿™ä¸ªæ‰“å­—æ€ä¹ˆå…¨éƒ¨éƒ½æ˜¯æ‹¼éŸ³.", 
   "æœ‰è°çŸ¥é“æˆ‘ç©ºé—´å¼€å¤´åŠ¨ç”»çš„éŸ³ä¹æ¥è‡ªå“ªé‡Œï¼Œæœ‰ä¸‹è½½åœ°å€å—?", "åŒå­¦ä»¬å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯æ ¡åŒ»é™¢æ¨åŒ»ç”Ÿ.", 

]

# css to make videos look nice
# var(--block-border-color); TODO
CSS = """
.generate_video {
    position: relative;
    margin-left: auto;
    margin-right: auto;
    box-shadow: var(--block-shadow);
    border-width: var(--block-border-width);
    border-color: #000000;
    border-radius: var(--block-radius);
    background: var(--block-background-fill);
    width: 25%;
    line-height: var(--line-sm);
}
}
"""
DEFAULT_TEXT = "è¾“å…¥æƒ³è¦ç”Ÿæˆçš„æ–‡æœ¬ "

if not os.path.exists("./data/stats"):
    os.makedirs("./data/stats")
    with open("./data/stats/Prompts.text", 'w') as f: #æ‰“å¼€æ–‡ä»¶ï¼Œä¿å­˜æç¤º
        pass

Total_Calls = 4730
def update_total_calls():
    global Total_Calls
    Total_Calls_offset = 4730 ## init number from visit, 01/07
    with open("./data/stats/Prompts.text", 'r') as f:
        Total_Calls = len(f.readlines()) + Total_Calls_offset
    print("Prompts Num:",Total_Calls)

cached_dir = './cached'
uid = 1822
out_video_path = pjoin(cached_dir, f'{uid}') #è§†é¢‘è¾“å‡ºè·¯å¾„
os.makedirs(out_video_path, exist_ok=True)


#é…ç½®å‚æ•°
#è·å–å‘½ä»¤è¡Œå‚æ•°ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è„šæœ¬åç§°ï¼Œåé¢çš„å‚æ•°æ˜¯ä¼ é€’ç»™è„šæœ¬çš„æ–‡æœ¬æˆ–æ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„

parser = argparse.ArgumentParser(description='Convert text to speech using gTTS.')
parser.add_argument('--text', type=str, help='Text to convert to speech')
parser.add_argument('--output_audio', type=str, default='/data2/jingyc/project1/Wav2Lip/audio/converted_audio.mp3', help='Output path for the audio file')
parser.add_argument('--checkpoint_path', type=str, default='/data2/jingyc/project1/Wav2Lip/checkpoints/wav2lip.pth', help='checkpoint path for model')
parser.add_argument("--face_path", type=str, default='/data2/jingyc/project1/Wav2Lip/video/woman_cut.mp4', help="face path")
parser.add_argument('--outfile', type=str, help='Video path to save result. See default for an e.g.', 
                            default='/data2/jingyc/project1/Wav2Lip/results/debug.mp4')
parser.add_argument(        
    '-i',
    '--input',
    type=str,
    default='inputs/whole_imgs',
    help='Input image or folder. Default: inputs/whole_imgs')
parser.add_argument('-o', '--output', type=str, default='results', help='Output folder. Default: results')    
parser.add_argument(
    '-v', '--version', type=str, default='1.3', help='GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3')
parser.add_argument(
    '-s', '--upscale', type=int, default=2, help='The final upsampling scale of the image. Default: 2')
args = parser.parse_args()

wav2lip_path = '/data2/jingyc/project1/Wav2Lip'
gfpgan_path = '/data2/jingyc/project1/GFPGAN'

#éŸ³é¢‘è·¯å¾„
audio_path = args.output_audio




# pdb.set_trace()
def generate(
        text, 
        uid = uid,
        repeat_times=1,
        # args = args,
):
    # è¾“å…¥æ–‡æœ¬å’Œè§†é¢‘ï¼Œç”Ÿæˆè§†é¢‘ã€‚
    print("11111")
    print(text)
    with open("/data2/jingyc/project1/data/stats/Prompts.text", 'a') as f:
        f.write(text+'\n')
    update_total_calls()
    text_list = []
    text_list.append(text)
    # captions = text_list
    datas = []
    # text = text

    for r in range(repeat_times):

        ruid = random.randrange(999999999)

        #æ–‡æœ¬è½¬éŸ³é¢‘ï¼Œ éŸ³é¢‘ç”Ÿæˆlip
        if not text:
            print("è¯·æä¾›è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬æˆ–æ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„å‚æ•°ã€‚")
        else:
            wav2lip_path = '/data2/jingyc/project1/Wav2Lip'
            # os.chdir(wav2lip_path)
            # current_directory = os.getcwd()
            # print("å½“å‰å·¥ä½œç›®å½•:", current_directory)
            # target_path = "./audio/converted_audio.mp3"

            args.outfile = pjoin('/data2/jingyc/project1/Wav2Lip/results/', "sample_repeat%d_%d.mp4"%(r, ruid) )
            # å¦‚æœ text_argument æ˜¯ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹
            if os.path.isfile(text):
                with open(text, 'r', encoding='utf-8') as file:
                    text_content = file.read()
                text_to_speech(text_content, audio_path)
                print(f"æ–‡æœ¬æ–‡ä»¶ '{text}' ä¸­çš„å†…å®¹å·²è½¬æ¢ä¸ºè¯­éŸ³å¹¶ä¿å­˜åˆ° '{audio_path}'ã€‚")
                print("ç”Ÿæˆè§†é¢‘:")
                run_inference1_command(args.checkpoint_path, args.face_path, audio_path, args.outfile, wav2lip_path)
                print(f"è¯­éŸ³ '{audio_path}'å·²é©±åŠ¨å¹¶ç”Ÿæˆè§†é¢‘ï¼Œä¿å­˜åˆ° '{args.outfile}'")
            else:
                # å¦åˆ™ï¼Œå‡å®š text_argument æ˜¯æ–‡æœ¬å†…å®¹ï¼Œç›´æ¥è°ƒç”¨ text_to_speech å‡½æ•°
                text_to_speech(text, audio_path)
                print(f"æ–‡æœ¬ '{text}' å·²è½¬æ¢ä¸ºè¯­éŸ³å¹¶ä¿å­˜åˆ° '{audio_path}'ã€‚")
                print("ç”Ÿæˆè§†é¢‘:")
                run_inference1_command(args.checkpoint_path, args.face_path, audio_path, args.outfile, wav2lip_path)
                print(f"è¯­éŸ³ '{audio_path}'å·²é©±åŠ¨å¹¶ç”Ÿæˆè§†é¢‘ï¼Œä¿å­˜åˆ° '{args.outfile}'")
        
        
        video = args.outfile
                
        # é’ˆå¯¹ä¸Šé¢ç”Ÿæˆçš„è§†é¢‘è¿›è¡Œè¶…åˆ†

        directory_path, filename = os.path.split(args.outfile)
        filename_without_extension, extension = os.path.splitext(filename)
        gfpgan_path = '/data2/jingyc/project1/GFPGAN'
        whole_images = os.path.join(gfpgan_path, 'inputs/whole_imgs', filename_without_extension)
        # print(whole_images)
        if not os.path.exists(whole_images):
            os.makedirs(whole_images)
            print(f"ç›®å½• {whole_images} åˆ›å»ºæˆåŠŸã€‚")
        output_images = os.path.join(whole_images, 'output_%04d.png')
        #è§†é¢‘è½¬å¸§
        video2images(video, output_images)

        #å‡†å¤‡è¶…åˆ†
        # os.chdir(gfpgan_path)
        # current_directory = os.getcwd()
        # print("å½“å‰å·¥ä½œç›®å½•:", current_directory)
        #æ–°å»ºè¾“å‡ºå¸§çš„ç›®å½•
        out_img_path = os.path.join(gfpgan_path, args.output, filename_without_extension )
        if not os.path.exists(out_img_path):
            os.makedirs(out_img_path)
            print(f"ç›®å½• {out_img_path} åˆ›å»ºæˆåŠŸã€‚")

        # è¿›è¡Œè¶…åˆ†
        print("è¿›è¡Œè¶…åˆ†")
        run_inference2_command(whole_images, out_img_path, args.version, args.upscale, gfpgan_path)
        after_FR_images = os.path.join(out_img_path, 'restored_imgs', 'output_%04d.png')
        output_video_path = os.path.join(out_img_path, 'output_video.mp4')

        #å¸§è½¬è§†é¢‘
        images2video(after_FR_images, audio_path, output_video_path)
        print("è¶…åˆ†åè§†é¢‘ä¿å­˜ç›®å½•", output_video_path)
        data_path = pjoin(cached_dir,f'{uid}',"sample_repeat%d_%d.mp4"%(r, ruid))
        #å¤åˆ¶åˆ°cached_dirä¸­
        shutil.copy(output_video_path, data_path)
        data_unit = {
            "url": data_path
            }
        datas.append(data_unit)
    return datas


# run_inference1_command(args.checkpoint_path, args.face_path, audio_path, args.outfile, wav2lip_path)
# print(f"è¯­éŸ³ '{audio_path}'å·²é©±åŠ¨å¹¶ç”Ÿæˆè§†é¢‘ï¼Œä¿å­˜åˆ° '{args.outfile}'")
# æµ‹è¯•
# output_datas = []
# output_datas = generate("åŒå­¦ä»¬å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯æ ¡åŒ»é™¢æ¨è€å¸ˆã€‚")
# HTML component

def get_video_html(data, video_id, width=700, height=700):

    url = data["url"]  # mp4æ–‡ä»¶è·¯å¾„
    # class="wrap default svelte-gjihhp hide"
    # <div class="contour_video" style="position: absolute; padding: 10px;">
    # width="{width}" height="{height}"
    print(f'url:{url}')
    video_html = f"""
<h2 style='text-align: center'>
<a href="http://localhost:8866/{url}" download="video.mp4"><b>MP4 Download</b></a>
</h2>
<video class="generate_video" width="{width}" height="{height}" style="center" preload="auto" muted playsinline onpause="this.load()"
autoplay loop disablepictureinpicture id="{video_id}">
  <source src="http://localhost:8866/{url}" type="video/mp4">
  Your browser does not support the video tag.
</video>
"""
    return video_html

def generate_component(generate_function, text):
    if text == "" or text is None:
        return [None for _ in range(1)]
    # uid = random.randrange(99999)
    datas = generate_function(text, uid)
    htmls = [get_video_html(data, idx) for idx, data in enumerate(datas)]
    return htmls


# LOADING

# DEMO
port = 8866
# æ„å»ºå¯åŠ¨ HTTP æœåŠ¡å™¨çš„å‘½ä»¤
command = f"python3 -m http.server {port}"
try:
    # ä½¿ç”¨ subprocess è¿è¡Œå‘½ä»¤
    process = subprocess.Popen(command, shell=True)
except Exception as e:
    print(f"An error occurred: {e}")

theme = gr.themes.Default(primary_hue="blue", secondary_hue="gray")
generate_and_show = partial(generate_component, generate)

with gr.Blocks(css=CSS, theme=theme) as demo:
    gr.Markdown(WEBSITE)
    videos = []

    with gr.Row():
        with gr.Column(scale=3):
            text = gr.Textbox(
                show_label=True,
                label="è¾“å…¥æƒ³è¦ç”Ÿæˆçš„æ–‡æœ¬",
                value=DEFAULT_TEXT,
            )
            # with gr.Row():
            #     with gr.Column(scale=1):
            #         motion_len = gr.Textbox(
            #             show_label=True,
            #             label="Motion length (<10s)",
            #             value=0,
            #             info="Specify the motion length; 0 to use the default auto-setting.",
            #         )
            #     with gr.Column(scale=1):
            #         use_ik = gr.Radio(
            #             ["Raw", "IK"],
            #             label="Post-processing",
            #             value="IK",
            #             info="Use basic inverse kinematic (IK) for foot contact locking",
            #         )
            gen_btn = gr.Button("Generate", variant="primary")
            clear = gr.Button("Clear", variant="secondary")
            gr.Markdown(
                        f"""
                            
                        """
                    )

        with gr.Column(scale=2):

            def generate_example(text):
                return generate_and_show(text)

            examples = gr.Examples(
                examples=[[x] for x in EXAMPLES],
                inputs=[text],
                examples_per_page=10,
                run_on_click=False,
                cache_examples=False,
                fn=generate_example,
                outputs=[],
            )

    i = -1
    # should indent
    for _ in range(1):
        with gr.Row():
            for _ in range(1):
                i += 1
                video = gr.HTML()
                videos.append(video)
    gr.Markdown(WEBSITE_bottom)
    # connect the examples to the output
    # a bit hacky
    examples.outputs = videos

    def load_example(example_id):
        processed_example = examples.non_none_processed_examples[example_id]
        return gr.utils.resolve_singleton(processed_example)

    examples.dataset.click(
        load_example,
        inputs=[examples.dataset],
        outputs=examples.inputs_with_examples,  # type: ignore
        show_progress=False,
        postprocess=False,
        queue=False,
    ).then(fn=generate_example, inputs=examples.inputs, outputs=videos)

    gen_btn.click(  #ç”Ÿæˆ
        fn=generate_and_show,
        inputs=[text],
        outputs=videos,
    )
    text.submit(   #æäº¤æ–‡æœ¬
        fn=generate_and_show,
        inputs=[text],
        outputs=videos,
    )

    def clear_videos():
        return [None for x in range(1)] + [DEFAULT_TEXT]

    clear.click(fn=clear_videos, outputs=videos + [text])  #æ¸…é™¤

demo.launch(debug=True)





